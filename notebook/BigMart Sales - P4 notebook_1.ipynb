{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<a id=\"install\"></a>\n## Package installation\nBefore you use the sample code in this notebook, install the following packages:\n - ibm_watson_machine_learning,\n - autoai-libs,\n - xgboost.\n"}, {"metadata": {"execution": {"iopub.execute_input": "2020-10-12T14:00:45.009458Z", "iopub.status.busy": "2020-10-12T14:00:45.007968Z", "iopub.status.idle": "2020-10-12T14:00:46.037702Z", "shell.execute_reply": "2020-10-12T14:00:46.038270Z"}, "pycharm": {"name": "#%%\n"}, "scrolled": true}, "cell_type": "code", "source": "!pip install ibm-watson-machine-learning | tail -n 1\n!pip install -U autoai-libs==1.12.5 | tail -n 1\n!pip install -U xgboost==0.90 | tail -n 1", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm-watson-machine-learning) (0.15.2)\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from gensim==3.8.3->autoai-libs==1.12.5) (5.0.0)\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied, skipping upgrade: scipy in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from xgboost==0.90) (1.5.0)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"variables_definition\"></a>\n## AutoAI experiment metadata\nThe following cell contains the training data connection details.  \n**Note**: The connection might contain authorization credentials, so be careful when sharing the notebook."}, {"metadata": {"execution": {"iopub.execute_input": "2020-10-12T14:00:49.797633Z", "iopub.status.busy": "2020-10-12T14:00:49.796778Z", "iopub.status.idle": "2020-10-12T14:00:57.182715Z", "shell.execute_reply": "2020-10-12T14:00:57.183132Z"}, "pycharm": {"is_executing": true}}, "cell_type": "code", "source": "# @hidden_cell\nfrom ibm_watson_machine_learning.helpers import DataConnection\nfrom ibm_watson_machine_learning.helpers import S3Connection, S3Location\n\ntraining_data_reference = [DataConnection(\n    connection=S3Connection(\n        api_key='-0hzj7D4OYBdBfbMEuABibpFJU5NIGlHcp25FnWTksIg',\n        auth_endpoint='https://iam.bluemix.net/oidc/token/',\n        endpoint_url='https://s3-api.us-geo.objectstorage.softlayer.net'\n    ),\n        location=S3Location(\n        bucket='bigmartsates-donotdelete-pr-zivrdoivomqhzt',\n        path='BigMart_dataset.csv'\n    )),\n]\ntraining_result_reference = DataConnection(\n    connection=S3Connection(\n        api_key='-0hzj7D4OYBdBfbMEuABibpFJU5NIGlHcp25FnWTksIg',\n        auth_endpoint='https://iam.bluemix.net/oidc/token/',\n        endpoint_url='https://s3-api.us-geo.objectstorage.softlayer.net'\n    ),\n    location=S3Location(\n        bucket='bigmartsates-donotdelete-pr-zivrdoivomqhzt',\n        path='auto_ml/f4b0bf2d-413c-4859-ac33-6cc112e64f38/wml_data/c09a97ed-c4b8-484c-8ed1-575227e10eeb/data/automl',\n        model_location='auto_ml/f4b0bf2d-413c-4859-ac33-6cc112e64f38/wml_data/c09a97ed-c4b8-484c-8ed1-575227e10eeb/data/automl/hpo_c_output/Pipeline1/model.pickle',\n        training_status='auto_ml/f4b0bf2d-413c-4859-ac33-6cc112e64f38/wml_data/c09a97ed-c4b8-484c-8ed1-575227e10eeb/training-status.json'\n    ))", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Following cell contains input parameters provided to run the AutoAI experiment in Watson Studio."}, {"metadata": {"execution": {"iopub.execute_input": "2020-10-12T14:00:57.187305Z", "iopub.status.busy": "2020-10-12T14:00:57.186602Z", "iopub.status.idle": "2020-10-12T14:00:57.188392Z", "shell.execute_reply": "2020-10-12T14:00:57.188878Z"}, "pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "experiment_metadata = dict(\n   prediction_type='regression',\n   prediction_column='Item_Outlet_Sales',\n   holdout_size=0.1,\n   scoring='neg_root_mean_squared_error',\n   deployment_url='https://us-south.ml.cloud.ibm.com',\n   csv_separator=',',\n   random_state=33,\n   max_number_of_estimators=2,\n   daub_include_only_estimators=None,\n   training_data_reference=training_data_reference,\n   training_result_reference=training_result_reference,\n   project_id='c8fa43cd-5d3d-4fd1-aac5-61b45c83ece5'\n)", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"inspection\"></a>\n# Pipeline inspection"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"read\"></a>\n## Read training data\n\nRetrieve training dataset from AutoAI experiment as pandas DataFrame."}, {"metadata": {"execution": {"iopub.execute_input": "2020-10-12T14:01:16.076169Z", "iopub.status.busy": "2020-10-12T14:01:16.075589Z", "iopub.status.idle": "2020-10-12T14:01:19.190233Z", "shell.execute_reply": "2020-10-12T14:01:19.190807Z"}, "pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "df = training_data_reference[0].read(csv_separator=experiment_metadata['csv_separator'])\ndf.dropna('rows', how='any', subset=[experiment_metadata['prediction_column']], inplace=True)", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"split\"></a>\n##  Train and test data split"}, {"metadata": {"pycharm": {"is_executing": true}}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\n\ndf.drop_duplicates(inplace=True)\nX = df.drop([experiment_metadata['prediction_column']], axis=1).values\ny = df[experiment_metadata['prediction_column']].values\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=experiment_metadata['holdout_size'],\n                                                    random_state=experiment_metadata['random_state'])", "execution_count": 18, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"preview_model_to_python_code\"></a>\n## Make pipeline\nIn the next cell, you can find the Scikit-learn definition of the selected AutoAI pipeline."}, {"metadata": {}, "cell_type": "markdown", "source": "Import statements."}, {"metadata": {"pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "from autoai_libs.transformers.exportable import NumpyColumnSelector\nfrom autoai_libs.transformers.exportable import CompressStrings\nfrom autoai_libs.transformers.exportable import NumpyReplaceMissingValues\nfrom autoai_libs.transformers.exportable import NumpyReplaceUnknownValues\nfrom autoai_libs.transformers.exportable import boolean2float\nfrom autoai_libs.transformers.exportable import CatImputer\nfrom autoai_libs.transformers.exportable import CatEncoder\nimport numpy as np\nfrom autoai_libs.transformers.exportable import float32_transform\nfrom sklearn.pipeline import make_pipeline\nfrom autoai_libs.transformers.exportable import FloatStr2Float\nfrom autoai_libs.transformers.exportable import NumImputer\nfrom autoai_libs.transformers.exportable import OptStandardScaler\nfrom sklearn.pipeline import make_union\nfrom autoai_libs.transformers.exportable import NumpyPermuteArray\nfrom autoai_libs.cognito.transforms.transform_utils import TAM\nfrom sklearn.decomposition import PCA\nfrom autoai_libs.cognito.transforms.transform_utils import FS1\nfrom autoai_libs.cognito.transforms.transform_utils import TA1\nimport autoai_libs.utils.fc_methods\nfrom xgboost import XGBRegressor", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Pre-processing & Estimator."}, {"metadata": {"pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "numpy_column_selector_0 = NumpyColumnSelector(\n    columns=[0, 2, 4, 6, 7, 8, 9, 10]\n)\ncompress_strings = CompressStrings(\n    compress_type=\"hash\",\n    dtypes_list=[\n        \"char_str\",\n        \"char_str\",\n        \"char_str\",\n        \"char_str\",\n        \"int_num\",\n        \"char_str\",\n        \"char_str\",\n        \"char_str\",\n    ],\n    missing_values_reference_list=[\"\", \"-\", \"?\", float(\"nan\")],\n    misslist_list=[[], [], [], [], [], [float(\"nan\")], [], []],\n)\nnumpy_replace_missing_values_0 = NumpyReplaceMissingValues(\n    missing_values=[float(\"nan\")], filling_values=float(\"nan\")\n)\nnumpy_replace_unknown_values = NumpyReplaceUnknownValues(\n    filling_values=float(\"nan\"),\n    filling_values_list=[\n        float(\"nan\"),\n        float(\"nan\"),\n        float(\"nan\"),\n        float(\"nan\"),\n        float(\"nan\"),\n        float(\"nan\"),\n        float(\"nan\"),\n        float(\"nan\"),\n    ],\n    missing_values_reference_list=[\"\", \"-\", \"?\", float(\"nan\")],\n)\ncat_imputer = CatImputer(\n    strategy=\"most_frequent\",\n    missing_values=float(\"nan\"),\n    sklearn_version_family=\"23\",\n)\ncat_encoder = CatEncoder(\n    encoding=\"ordinal\",\n    categories=\"auto\",\n    dtype=np.float64,\n    handle_unknown=\"error\",\n    sklearn_version_family=\"23\",\n)\npipeline_0 = make_pipeline(\n    numpy_column_selector_0,\n    compress_strings,\n    numpy_replace_missing_values_0,\n    numpy_replace_unknown_values,\n    boolean2float(),\n    cat_imputer,\n    cat_encoder,\n    float32_transform(),\n)\nnumpy_column_selector_1 = NumpyColumnSelector(columns=[1, 3, 5])\nfloat_str2_float = FloatStr2Float(\n    dtypes_list=[\"float_num\", \"float_num\", \"float_num\"],\n    missing_values_reference_list=[float(\"nan\")],\n)\nnumpy_replace_missing_values_1 = NumpyReplaceMissingValues(\n    missing_values=[float(\"nan\")], filling_values=float(\"nan\")\n)\nnum_imputer = NumImputer(strategy=\"median\", missing_values=float(\"nan\"))\nopt_standard_scaler = OptStandardScaler(\n    num_scaler_copy=None,\n    num_scaler_with_mean=None,\n    num_scaler_with_std=None,\n    use_scaler_flag=False,\n)\npipeline_1 = make_pipeline(\n    numpy_column_selector_1,\n    float_str2_float,\n    numpy_replace_missing_values_1,\n    num_imputer,\n    opt_standard_scaler,\n    float32_transform(),\n)\nunion = make_union(pipeline_0, pipeline_1)\nnumpy_permute_array = NumpyPermuteArray(\n    axis=0, permutation_indices=[0, 2, 4, 6, 7, 8, 9, 10, 1, 3, 5]\n)\ntam = TAM(\n    tans_class=PCA(),\n    name=\"pca\",\n    col_names=[\n        \"Item_Identifier\",\n        \"Item_Weight\",\n        \"Item_Fat_Content\",\n        \"Item_Visibility\",\n        \"Item_Type\",\n        \"Item_MRP\",\n        \"Outlet_Identifier\",\n        \"Outlet_Establishment_Year\",\n        \"Outlet_Size\",\n        \"Outlet_Location_Type\",\n        \"Outlet_Type\",\n    ],\n    col_dtypes=[\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n    ],\n)\nfs1_0 = FS1(\n    cols_ids_must_keep=range(0, 11),\n    additional_col_count_to_keep=12,\n    ptype=\"regression\",\n)\nta1 = TA1(\n    fun=np.sqrt,\n    name=\"sqrt\",\n    datatypes=[\"numeric\"],\n    feat_constraints=[\n        autoai_libs.utils.fc_methods.is_non_negative,\n        autoai_libs.utils.fc_methods.is_not_categorical,\n    ],\n    col_names=[\n        \"Item_Identifier\",\n        \"Item_Weight\",\n        \"Item_Fat_Content\",\n        \"Item_Visibility\",\n        \"Item_Type\",\n        \"Item_MRP\",\n        \"Outlet_Identifier\",\n        \"Outlet_Establishment_Year\",\n        \"Outlet_Size\",\n        \"Outlet_Location_Type\",\n        \"Outlet_Type\",\n        \"pca_0\",\n        \"pca_1\",\n        \"pca_2\",\n        \"pca_3\",\n        \"pca_4\",\n        \"pca_5\",\n        \"pca_6\",\n        \"pca_7\",\n        \"pca_8\",\n        \"pca_9\",\n        \"pca_10\",\n    ],\n    col_dtypes=[\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n        np.dtype(\"float32\"),\n    ],\n)\nfs1_1 = FS1(\n    cols_ids_must_keep=range(0, 11),\n    additional_col_count_to_keep=12,\n    ptype=\"regression\",\n)\nxgb_regressor = XGBRegressor(\n    gamma=0.7258542214206876,\n    learning_rate=0.02,\n    min_child_weight=20,\n    n_estimators=319,\n    n_jobs=2,\n    objective=\"reg:squarederror\",\n    random_state=33,\n    reg_alpha=0.32681018560472175,\n    reg_lambda=0.10155046152079605,\n    silent=True,\n    subsample=0.7444835249102951,\n    verbosity=0,\n)\n", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Pipeline."}, {"metadata": {"pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "pipeline = make_pipeline(\n    union, numpy_permute_array, tam, fs1_0, ta1, fs1_1, xgb_regressor\n)", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"train\"></a>\n## Train pipeline model\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Define scorer from the optimization metric\nThis cell constructs the cell scorer based on the experiment metadata."}, {"metadata": {"pycharm": {"is_executing": true}}, "cell_type": "code", "source": "from sklearn.metrics import get_scorer\n\nscorer = get_scorer(experiment_metadata['scoring'])", "execution_count": 22, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"test_model\"></a>\n### Fit pipeline model\nIn this cell, the pipeline is fitted."}, {"metadata": {"execution": {"iopub.execute_input": "2020-10-12T14:01:19.291734Z", "iopub.status.busy": "2020-10-12T14:01:19.244735Z", "iopub.status.idle": "2020-10-12T14:01:19.338461Z", "shell.execute_reply": "2020-10-12T14:01:19.338958Z"}, "pycharm": {"is_executing": true, "name": "#%%\n"}, "scrolled": true}, "cell_type": "code", "source": "pipeline.fit(train_X,train_y)", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "Pipeline(steps=[('featureunion',\n                 FeatureUnion(transformer_list=[('pipeline-1',\n                                                 Pipeline(steps=[('numpycolumnselector',\n                                                                  NumpyColumnSelector(columns=[0,\n                                                                                               2,\n                                                                                               4,\n                                                                                               6,\n                                                                                               7,\n                                                                                               8,\n                                                                                               9,\n                                                                                               10])),\n                                                                 ('compressstrings',\n                                                                  CompressStrings(compress_type='hash',\n                                                                                  dtypes_list=['char_str',\n                                                                                               'char_str',\n                                                                                               'char_str',\n                                                                                               'char_str',\n                                                                                               'int_num',\n                                                                                               'char_str',\n                                                                                               'char_str',\n                                                                                               'char_str'],\n                                                                                  missing_values_reference_lis...\n                 autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 11), additional_col_count_to_keep = 12, ptype = 'regression')),\n                ('xgbregressor',\n                 XGBRegressor(gamma=0.7258542214206876, learning_rate=0.02,\n                              min_child_weight=20, n_estimators=319, n_jobs=2,\n                              objective='reg:squarederror', random_state=33,\n                              reg_alpha=0.32681018560472175,\n                              reg_lambda=0.10155046152079605, silent=True,\n                              subsample=0.7444835249102951, verbosity=0))])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"test_model\"></a>\n## Test pipeline model"}, {"metadata": {}, "cell_type": "markdown", "source": "Score the fitted pipeline with the generated scorer using the holdout dataset."}, {"metadata": {"execution": {"iopub.execute_input": "2020-10-12T14:02:03.910267Z", "iopub.status.busy": "2020-10-12T14:02:03.909710Z", "iopub.status.idle": "2020-10-12T14:02:03.914154Z", "shell.execute_reply": "2020-10-12T14:02:03.914727Z"}, "pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "score = scorer(pipeline, test_X, test_y)\nprint(score)", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "-1107.167785383792\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"next_steps\"></a>\n# Next steps\n\n#### [Model deployment as webservice](https://github.com/IBM/watson-machine-learning-samples/tree/master/cloud/notebooks/python_sdk/deployments/autoai)\n#### [Run AutoAI experiment with python SDK](https://github.com/IBM/watson-machine-learning-samples/tree/master/cloud/notebooks/python_sdk/experiments/autoai)  "}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2021 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs  \n(or equivalent) and License Information document for Watson Studio Auto-generated Notebook (License Terms),  \nsuch agreements located in the link below. Specifically, the Source Components and Sample Materials clause  \nincluded in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"http://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\">License Terms</a>\n\n___"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}